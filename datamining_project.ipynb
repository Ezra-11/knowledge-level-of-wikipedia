{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ezra-11/knowledge-level-of-wikipedia/blob/main/datamining_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Business Understanding\n",
        "\n",
        "## 1.1 Problem Statement\n",
        "Wikipedia content about *Zambia* remains significantly underrepresented, with very few comprehensive pages dedicated to Zambian topics, history, culture, and current affairs. The *DataLab Research group* at The University of Zambia has identified that understanding the knowledge levels and expertise of current Wikipedia contributors working on Zambian content is crucial for developing targeted strategies to improve content quality and quantity.\n",
        "\n",
        "Currently, there is no systematic way to assess the expertise and knowledge depth of contributors to Zambian Wikipedia pages, making it difficult to:\n",
        "\n",
        "- Identify subject matter experts who could mentor new contributors  \n",
        "- Match contributors with appropriate content areas based on their expertise  \n",
        "- Develop targeted training programs for contributors with different knowledge levels  \n",
        "- Optimize content review and quality assurance processes  \n",
        "\n",
        "*Core Problem:* How can we systematically classify and understand the knowledge levels of Wikipedia contributors working on Zambian content to improve the overall quality and quantity of Wikipedia pages about Zambia?"
      ],
      "metadata": {
        "id": "QX4t4wZizW16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "w4GcaRvn9YdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GHCCXc0Sm8gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kqFS6P2CCnmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Business Objectives\n",
        "\n",
        "### Primary Objectives:\n",
        "- *Enhance Content Quality:* Improve the accuracy, depth, and comprehensiveness of Wikipedia articles about Zambia by better understanding contributor expertise levels  \n",
        "- *Optimize Contributor Engagement:* Develop targeted strategies to engage contributors based on their knowledge levels and areas of expertise  \n",
        "- *Facilitate Knowledge Transfer:* Identify expert contributors who can mentor newcomers and provide guidance on complex topics  \n",
        "- *Improve Content Coverage:* Strategically assign content creation and improvement tasks based on contributor knowledge levels  \n",
        "\n"
      ],
      "metadata": {
        "id": "x6nkA0Gd2dfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Secondary Objectives:\n",
        "- **Build Community Capacity:** Create a framework for assessing and developing Wikipedia contributor skills within the Zambian context  \n",
        "- ***Inform Training Programs:*** Develop evidence-based training curricula tailored to different contributor knowledge levels  \n",
        "- ***Support Academic Research:*** Provide insights into digital knowledge creation patterns in developing countries  \n",
        "\n",
        "### Success Metrics:\n",
        "- *Short-term:* Accurate classification of contributor knowledge levels with measurable confidence scores  \n",
        "- *Medium-term:* Increased quality scores of Zambian Wikipedia articles (measured by completeness, references, and peer ratings)  \n",
        "- ***Long-term:*** Growth in the number of active Zambian Wikipedia contributors and articles about Zambia  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "__c5mcIc0yaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d0f69ddN21JS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.4 Project Success Criteria**\n",
        "\n",
        "###Model Performance Criteria\n",
        "- *Primary Metric:* Classification accuracy of at least *80%* on a held-out test set  \n",
        "- *Precision and Recall:* Minimum *75%* precision and recall for each knowledge level category  \n",
        "- *F1-Score:* Overall weighted F1-score of at least *0.78*  \n",
        "- *Cross-validation Stability:* Model performance should not vary by more than *5%* across different data splits  \n",
        "\n",
        "###Business Impact Criteria\n",
        "- *Expert Validation:* At least *85%* agreement between model classifications and manual assessments by domain experts  \n",
        "- *Actionability:* Classification results must provide clear, actionable insights for contributor development strategies  \n",
        "- *Scalability:* Model should be able to process new contributors and updates efficiently (*< 1 minute* per contributor)  \n",
        "\n",
        "###Technical Criteria\n",
        "- *Interpretability:* Model decisions must be explainable with feature importance scores and decision reasoning  \n",
        "- *Robustness:* Model should maintain performance when applied to contributors from different time periods  \n",
        "- *Generalizability:* Framework should be adaptable to other developing country contexts or subject areas  \n",
        "\n",
        "###Data Quality Criteria\n",
        "- *Coverage:* Successfully classify at least *90%* of active contributors to Zambian Wikipedia pages  \n",
        "- *Consistency:* Inter-rater reliability (Kappa score) *> 0.7* for manual validation labels  \n",
        "- *Completeness:* Less than *10%* missing data across critical features  \n",
        "\n",
        "###Deployment Criteria\n",
        "- *Integration:* Model should be implementable within existing Wikipedia contributor management workflows  \n",
        "- *User Acceptance:* Positive feedback from at least *80%* of Wikipedia administrators and experienced contributors who test the system  \n",
        "- *Maintenance:* Established process for model updates and performance monitoring over time"
      ],
      "metadata": {
        "id": "31gXWIsqyaYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.3 Data Mining Goals**\n",
        "\n",
        "Primary Data Mining Goal:\n",
        "Build a *multi-class classification model* that can automatically categorize Wikipedia contributors working on Zambian content into distinct knowledge level categories (e.g., Novice, Intermediate, Advanced, Expert) based on their contribution patterns, edit quality, and content expertise.\n",
        "\n",
        "###Specific Technical Objectives\n",
        "\n",
        "Feature Engineering\n",
        "Extract meaningful features from contributor data including:\n",
        "- Edit frequency and consistency patterns  \n",
        "- Quality indicators (citations added, grammar improvements, factual accuracy)  \n",
        "- Content complexity (technical terms, specialized knowledge areas)  \n",
        "- Collaboration patterns (talk page participation, peer interactions)  \n",
        "- Longevity and commitment indicators  \n",
        "\n",
        "###Classification Model Development\n",
        "- Implement and compare multiple classification algorithms (Random Forest, SVM, Neural Networks, etc.)  \n",
        "- Develop ensemble methods to improve classification accuracy  \n",
        "- Create interpretable models that explain why a contributor is classified at a particular level  \n",
        "\n",
        "###Clustering Analysis (Secondary)\n",
        "- Identify natural groupings of contributors based on their expertise areas and contribution styles  \n",
        "- Discover contributor archetypes that may not align with traditional knowledge level categories  \n",
        "\n",
        "###Predictive Modeling\n",
        "- Predict potential contributor knowledge growth trajectories  \n",
        "- Identify contributors likely to become long-term, high-quality contributors"
      ],
      "metadata": {
        "id": "Wdws93kv4YB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Understanding\n"
      ],
      "metadata": {
        "id": "ej7HHjKU9fPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ii2ndLft9_ut",
        "outputId": "62bc42a1-eaea-452d-eaaf-dea50896297d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"zambia_contributors_summary.csv\")\n",
        "# 1. Display first few rows\n",
        "print(\"First 10 rows:\")\n",
        "display(df.head(10))\n",
        "\n",
        "# Info about columns, data types, and non-null counts\n",
        "df.info()\n",
        "\n",
        "df.describe()\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "\n",
        "display(df.isnull().sum())\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "numerical_cols = ['Total_Edits', 'Pages_Edited', 'Total_Size', 'Active_Days', 'Edits_per_Month']\n",
        "\n",
        " # Histograms\n",
        "df[numerical_cols].hist(figsize=(12,8), bins=15, color='skyblue')\n",
        "plt.suptitle(\"Distribution of Numerical Columns\")\n",
        "plt.show()\n",
        "\n",
        "# Boxplots for outlier detection\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.boxplot(data=df[numerical_cols])\n",
        "plt.title(\"Boxplot of Numerical Columns\")\n",
        "plt.show()\n",
        "\n",
        "top_contributors = df.sort_values('Total_Edits', ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x='Contributor', y='Total_Edits', data=top_contributors, palette='viridis')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Top 10 Contributors by Total Edits\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "F8qGjB7rQQIi",
        "outputId": "bf9ccee9-86dc-4da4-ff96-65a7a34094e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'zambia_contributors_summary.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2357431665.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"zambia_contributors_summary.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# 1. Display first few rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"First 10 rows:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'zambia_contributors_summary.csv'"
          ]
        }
      ]
    }
  ]
}